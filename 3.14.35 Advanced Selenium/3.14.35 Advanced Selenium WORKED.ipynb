{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7956d98b-e9d5-4f69-829a-abb9813b9459",
   "metadata": {},
   "source": [
    "# 3.14.35 Advanced Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff908407-92d9-42b2-9764-d9810eadbf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requred libraries and modules\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801ecfea-9c56-448e-83b2-6ad14352040d",
   "metadata": {},
   "source": [
    "## Pagination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feab432b-1d4c-4520-9e80-199ad6e2bdf2",
   "metadata": {},
   "source": [
    "Pagination is used in contexts beyond webscraping and broadly refers to splitting up a large piece of content into smaller sections (i.e., pages). For example, when you search something on Google the result list (which could be thousands or millions of results) is split up into smaller 'pages' than contain 20 results each. \n",
    "\n",
    "In webscraping, we often need to be able to write our script so that it can deal with pagination and navigate between multiple pages of results or data. In the `Script1_Pagination.py` file we'll find a script that allows us to use pagination to retrieve all the results from the [hockey teams webpage](https://www.scrapethissite.com/pages/forms/). Let's explore this in more detail below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17f07882-d58f-4568-882e-364bc572480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DRIVER SETUP\n",
    "\n",
    "# initialise the driver (and open up a browser window)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# open up a specific web page\n",
    "driver.get(\"https://www.scrapethissite.com/pages/forms/\")\n",
    "\n",
    "\n",
    "### DEFINE FUNCTION FOR COLUMN LIST CREATION\n",
    "\n",
    "# since we will need to repeat this process for other 5 variables, \n",
    "# let's create a function\n",
    "def create_col(class_name):\n",
    "    s_column = driver.find_elements(By.CLASS_NAME, class_name)\n",
    "    column = [col.text for col in s_column]\n",
    "    return column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f4ed0-2572-4029-9cf9-be3ed1931126",
   "metadata": {},
   "source": [
    "As with the previous lesson, our script opens with the initial set up of our Chrome webdriver and navigates to the page we want to scrape. We also set up our `create_col` function that we saw from the previous lesson. Previously, when we were only scraping one page of data, we used the `create_col` function to extract each column of data from the table. Seeing that we'll now need to do that for every single page on the website, we'll create another function called `scrape_page` to do all of this in one function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6fe1343-dac5-4704-8735-45980f220cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE FUNCTION FOR PAGE SCRAPING\n",
    "\n",
    "def scrape_page(page):\n",
    "    # click on first page\n",
    "    driver.find_element(By.LINK_TEXT, str(page)).click()\n",
    "    \n",
    "    # create a list for each column of the table\n",
    "    names = create_col('name')\n",
    "    year = create_col('year')\n",
    "    wins = create_col('wins')\n",
    "    losses = create_col('losses')\n",
    "    goals_for = create_col('gf')\n",
    "    goals_against = create_col('ga')\n",
    "    \n",
    "    # create a temporary DataFrame to store the current page\n",
    "    df_teams_tmp = pd.DataFrame(\n",
    "        {'names': names, \n",
    "         'year': year,\n",
    "         'wins': wins, \n",
    "         'losses': losses, \n",
    "         'goals_for': goals_for, \n",
    "         'goals_against': goals_against\n",
    "         })\n",
    "    \n",
    "    return df_teams_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281ac863-9b35-484e-9933-947e3d40ef42",
   "metadata": {},
   "source": [
    "Our `scrape_page` function takes in page as a parameter and then uses that to find the relevant link text and click on it. If we look at the [webpage we're scraping](https://www.scrapethissite.com/pages/forms/), you'll notice that each number on the page navigation at the bottom is actually a link that navigates to a specific page when clicked. Our function uses this to click on the relevant page link based on the parameter we pass into it, and uses our create_col function to scrape the data from that page and wrap it all up in a tidy dataframe. \n",
    "\n",
    "\n",
    "We could then use this function to scrape a specific page of data if we wanted to, like in the below code line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fbb00d8-47e9-4962-bb96-ba278adc268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teams = scrape_page(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0341ba-aebc-47f5-8534-53fd813fe281",
   "metadata": {},
   "source": [
    "However, in our case we want to scrape all of the pages! In order to do this, we first need a way of determining exactly how many pages of data there are on this webpage. If we look at the HTML code for our webpage, we'll see that the page navigation bar is contained within an unordered list `<ul>` tag with a class name of `pagination`. Let's start by first extracting this segment of HTML code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0e0176e-b50d-4298-a389-b07b939a184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pages = driver.find_element(By.CLASS_NAME, \"pagination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd6657-c789-41b5-8dcc-aaafaa0f6cee",
   "metadata": {},
   "source": [
    "If we look at the text of this HTML element, we'll see that it's a long string of all the page numbers, separated by the new line character `\\n`. We can use the split method to create a list of the page numbers by splitting on the new line characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4a7db7b-6c56-456a-abc5-d54145a38fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n»'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_pages.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3be0ed0f-e1ce-49ac-9a69-ade02cab8108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '»']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_pages.text.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eb02b1-1f2e-4bda-8686-06130e210cd2",
   "metadata": {},
   "source": [
    "Great! We've nearly got our complete list of numbers. However, you'll notice that the very last element of our list actually isn't a number but two arrows `>>`, which is the button on the navigation bar to move to the next page. This obviously won't work with our scrape_page function, so we'll adjust this list slightly to remove the last element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bf7330b-1434-4a05-b691-08acf835d425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of strings\n",
    "pages = s_pages.text.split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95647e4c-aa6b-404b-a450-3c6e52494aa9",
   "metadata": {},
   "source": [
    "We can now loop through this list of numbers and use them as arguments for our scrape_page function. We'll create an empty dataframe to hold all the data, and then at each loop we'll append (concat) the scraped page of data to this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "776984e5-c406-460c-aadd-c3f37066d086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise an empty dataframe to contain our data\n",
    "df_teams = pd.DataFrame([])\n",
    "\n",
    "# Loop through all of the pages and use the scrape page function to append the data to df_teams\n",
    "for p in pages: \n",
    "    # append (with pd.concat()) the temp DataFrame to the empty one\n",
    "    df_teams = pd.concat([df_teams, scrape_page(p)])\n",
    "    \n",
    "df_teams.reset_index(inplace=True, drop=True)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b665c21-6594-402f-9b1c-a861f34543b5",
   "metadata": {},
   "source": [
    "This is just one example of how we could paginate through a webpage. Other common ways of pagination are to use a while loop to check whether a \"next page button\" is present and continue to scrape the data until that button no longer appears (i.e., when you reach the final page) or sometimes websites will use a very structure url that has a page parameter (e.g., page=1, page=2 etc.) so we could directly update the webpage url."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0fb5f3-a4f1-4eb0-94d0-501b6094de93",
   "metadata": {},
   "source": [
    "## Dealing with Ajax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cb8be0-f18b-41e3-a3e5-c650d5754872",
   "metadata": {},
   "source": [
    "Ajax stands for \"**A**synchronous **Ja**vaScript And **X**ML\" and is a technique for transfering data between a server side database and a client side browser. It allows for faster, more dynamic and more interactive web pages. \n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*6PasVp89PTHbDvYJ-Zq8ZQ.png\" width=\"600\" style=\"display: block; margin: 0 auto\">\n",
    "\n",
    "\n",
    "\n",
    "Most of the webpages we have seen so far have been loaded using **synchronous requests** - that is, all of the HTML content for the webpage is loaded in one go the first time we send a request to the web url. With **asynchronous requests**, a request is sent to the web server and then the current page is updated without having to reload the page or be redirected to another web url. This introduces some problems for webscraping as we need a way to wait until the ajax request has been completed and the new data has been loaded onto the webpage before we can go ahead and start scraping it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd33dff3-0a68-4737-b443-01add71a51c4",
   "metadata": {},
   "source": [
    "Let's start with setting up our Chrome webdriver and navigating to this [link](https://www.scrapethissite.com/pages/ajax-javascript/) from the scrapethissite.com website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a95b034-fbaf-433b-ac25-1f66e5545273",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.scrapethissite.com/pages/ajax-javascript/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb07809d-9c4d-4790-841b-0393cbbab2af",
   "metadata": {},
   "source": [
    "As you'll see on the webpage, the page contains a series of clickable links for different years. When we click these links, we see a brief loading wheel before a table with the top films for that year is displayed. \n",
    "\n",
    "Behind the scenes, when we click on one of the year links the website is sending an ajax request to it's database to retrieve the relevent data and then loading it to the screen. If we were to run our code normally to try and retrieve this data, we would run into some problems (see the code below!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e42fe91-e8f2-427d-a7ff-95780326a37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def create_col(class_name):\n",
    "    s_column = driver.find_elements(By.CLASS_NAME, class_name)\n",
    "    column = [col.text for col in s_column]\n",
    "    return column\n",
    "\n",
    "\n",
    "driver.find_element(By.LINK_TEXT, \"2015\").click()\n",
    "film_titles = create_col(\"film-title\")\n",
    "\n",
    "print(film_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740f8bb6-2ec0-49cb-a511-f9ac6a222134",
   "metadata": {},
   "source": [
    "As you can see, what we end up with is an empty list. What's happening here is that our code is running faster than the website can load the relevant data, so when our code executes the line `film_titles = create_col(\"film-title\")`, that part of the page hasn't actually been loaded in yet, and so we end up with an empty list. What we need is a way for our code to wait until the relevant data has been loaded before it continues with the rest of the code.\n",
    "\n",
    "Fortunately, selenium provides a number of built in functions that can make our code wait until certain conditions have been met. If we check the selenium documentation on [waits](https://selenium-python.readthedocs.io/waits.html), we'll see that selenium can make our code wait for certain events. There are a number of different [built in checks](https://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.support.expected_conditions) that we can do before our code continues.\n",
    "\n",
    "Depending on the webpage you're scraping a different check might be required. For example, we could check that table elements are currently displayed on the page before trying to scrpae them. In our case we'll do the reverse. Whenever we click one of the year buttons, a spinning wheel is briefly displayed. So we'll check that the spinning wheel is not visible first, before then proceeding to scrape the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a38ab893-9ab1-4f64-b794-7dbb936a339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some additional libraries\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b44ffbb-9554-428a-9ffc-7825ac989065",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.LINK_TEXT, \"2015\").click()\n",
    "WebDriverWait(driver, 10).until(EC.invisibility_of_element_located((By.ID, \"loading\")))\n",
    "film_titles = create_col(\"film-title\")\n",
    "\n",
    "film_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f1cddd-9ca0-4940-8ded-8232cf91a456",
   "metadata": {},
   "source": [
    "The above code block uses the `WebDriverWait` function to pause our code for a set number of seconds (in this case 10). We combine this with the `until` method to continiously pause our code until a relevant condition is met - in this case it will pause until the loading wheel is no longer shown on screen. We could chain this to loop through each button to retieve the full set of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f55f5bd6-b678-493d-a918-1e2b9458cd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>nominations</th>\n",
       "      <th>awards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010</td>\n",
       "      <td>The King's Speech</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010</td>\n",
       "      <td>Inception</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010</td>\n",
       "      <td>The Social Network</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010</td>\n",
       "      <td>The Fighter</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010</td>\n",
       "      <td>Toy Story 3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year               title nominations awards\n",
       "0  2010   The King's Speech          12      4\n",
       "1  2010           Inception           8      4\n",
       "2  2010  The Social Network           8      3\n",
       "3  2010         The Fighter           7      2\n",
       "4  2010         Toy Story 3           5      2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up browser\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.scrapethissite.com/pages/ajax-javascript/\")\n",
    "\n",
    "def create_col(class_name):\n",
    "    s_column = driver.find_elements(By.CLASS_NAME, class_name)\n",
    "    column = [col.text for col in s_column]\n",
    "    return column\n",
    "\n",
    "# List of years to loop through\n",
    "years = [\"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\"]\n",
    "\n",
    "#Initialise empty dataframe\n",
    "df_films = pd.DataFrame([])\n",
    "\n",
    "# Loop through years, click the relevant button, wait for the content to load\n",
    "# then create a temporary dataframe, which we then append to the main one\n",
    "for year in years:\n",
    "    driver.find_element(By.LINK_TEXT, year).click()\n",
    "    WebDriverWait(driver, 10).until(EC.invisibility_of_element_located((By.ID, \"loading\")))\n",
    "    \n",
    "    film_titles = create_col(\"film-title\")\n",
    "    film_noms = create_col(\"film-nominations\")\n",
    "    film_awards = create_col(\"film-awards\")\n",
    "    \n",
    "    temp_df = pd.DataFrame({\n",
    "    \"year\": year,\n",
    "    \"title\": film_titles,\n",
    "    \"nominations\": film_noms,\n",
    "    \"awards\": film_awards\n",
    "    \n",
    "    })\n",
    "\n",
    "    \n",
    "    df_films = pd.concat([df_films, temp_df])\n",
    "    \n",
    "df_films.head()   \n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16151331-1f55-443e-9534-e17474df065b",
   "metadata": {},
   "source": [
    "### Dealing with Infinite Scrolls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300962ea-fe08-4747-9776-c035710d2653",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*ADvLjzLzT7SELZDs.gif\" width=\"600\" style=\"display: block; margin: 0 auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d355fe65-885a-4175-a81e-34ac3ca53f81",
   "metadata": {},
   "source": [
    "We've probably all seen infinite scrolling before - most (if not all) of the big social media platforms use this technique to keep you engaged with their content. The basic idea is that all of the content of the webpage is not loaded in one go. Instead a small selection of content is loaded to the screen and as you scroll towards the end of the selection of content more is automatically loaded to the page, leading you to spend hours and hours scrolling through infinite social media posts!\n",
    "\n",
    "Again, this poses another problem for our webscraping skills - if we wanted to scrape all the data from a page that uses infinite scrolling, we need a way to simulate this scrolling behaviour. Fortunately, selenium can do this for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1710d6-1622-4d3f-83a8-1141805053fd",
   "metadata": {},
   "source": [
    "#### An Example with US Politicians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecd57cd-1d9f-4f69-b1e9-985099391c6c",
   "metadata": {},
   "source": [
    "The website [GovTrack](https://www.govtrack.us/) is a non-profit organisation that tries to make US congress and politics more transparent, open and accessible. It contains heaps of data and information on US politicians, such as voting history and their campaign funding. Today we'll use our webscraping skills to initially create a list of all of the [Democrat representatives](https://www.govtrack.us/congress/members/current#current_role_party=Democrat).\n",
    "\n",
    "\n",
    "Let's start with the usual steps of initialising our ChromeDriver and navigating to the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0523cba-5cfd-42b4-b1a8-6e35526673f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.govtrack.us/congress/members/current#current_role_party=Democrat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299729ee-b21b-4524-80d9-85b30f9f613c",
   "metadata": {},
   "source": [
    "The first thing you should notice is another problem we haven't discussed yet: **pop-ups**. These can be handled in selenium the same way as in real life, by clicking the big X button. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fbfc35d-dd6c-4dac-ad3a-c75767bba5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH, '//*[@id=\"followus_modal\"]/div/div/div[1]/button').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7240d7ef-9fe4-4721-9407-fbc50386e906",
   "metadata": {},
   "source": [
    "If we take a look at the Democrat Representatives webpage, we'll see that it uses an infinite scroll mechanic to display all of the Democrat representatives. This is where we'll need to get our selenium script to mimic scrolling behaviour. \n",
    "\n",
    "The way we do this is by executing some javascript commands within our web browser. The basic process is like this:\n",
    "\n",
    "- Find what the maximum scroll height of the webpage is.\n",
    "- Set our current scroll height to be the same as the maximum height as the page.\n",
    "- At this point some more content will load and the scroll height of the webpage will change. \n",
    "- Find out what the new height of the webpage is and repeat the process until the webpage doesn't get any bigger (or until you reach a specified page height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7042abcd-7f4b-41a8-b876-b5fc7093e1e4",
   "metadata": {},
   "source": [
    "For our selenium script to work, we'll make use of two key javascript lines:\n",
    "\n",
    "- `document.body.scrollHeight` - This gets the maximum scroll height of the page\n",
    "- `window.scrollTo()` - This function allows us to simulate the actual scrolling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9bd968-a1dc-427f-bb71-677aa16e5f93",
   "metadata": {},
   "source": [
    "The first thing to do is to find out what the current scroll height is. To run this through our automated web browser, we'll use the `execute_script` method from selenium:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d75cabb9-85a1-4fc5-89b9-982b88c62fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016\n"
     ]
    }
   ],
   "source": [
    "driver.execute_script(\"return document.body.scrollHeight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0002cb-b20a-4d76-894f-f6ac915a4ea9",
   "metadata": {},
   "source": [
    "The next thing to do will be to move our scroll bar to the maximum scroll height, which we can do with the scrollTo() javascript function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2945c055-662d-4081-8a07-18022522512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7fe966-a373-4f86-963f-834454c64384",
   "metadata": {},
   "source": [
    "At this point, the page has updated and loaded additional content, which means the scroll height has now changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fa306d0-c135-47c7-8a91-cf11c99ea7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3376"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.execute_script(\"return document.body.scrollHeight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa50a88-a66a-4006-937d-66c8addfd4fb",
   "metadata": {},
   "source": [
    "What we need then is to create a loop to continue scrolling until the scroll height no longer increases (i.e., we've reached the bottom of the page). To do this we'll make use of a while loop, and at each point compare the new scroll height to the previous scroll height. We'll also see another (simpler) way to get our code to wait until things have loaded, and that's with the `time` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4bf263-c8ee-46c8-95bb-addc18e4a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e22c5d0-f766-4249-b41e-51c6322ad111",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    # Scroll to the bottom of the page\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "    \n",
    "    # Add in a brief wait for the content to load \n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Calculate the new scroll height and compare it with the previous one\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    # If both the last and new height are the same, we've reached the bottom and can exit the loop\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    \n",
    "    # Update the value of the last_height with the new height ready to begin the loop again\n",
    "    last_height = new_height"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4081ab8d-1860-4585-9eff-a1577c6c7acd",
   "metadata": {},
   "source": [
    "Perfect! We've now managed to scroll to the bottom of the page and can now proceed with scraping all of the Democrat names. We'll first of all filter the HTML content to the container of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64204961-d659-4c43-ad74-065d0eb2df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = driver.find_element(By.CLASS_NAME, \"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c1c5e3-b2e7-49e9-9577-328369452973",
   "metadata": {},
   "source": [
    "From this we'll create a result set of all the url links (the < a > tags) which we can then use our list comprehensions to extract the names and their associated links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37e739b3-d20f-4875-b2cc-bf8a6bb89695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the result set\n",
    "people = results.find_elements(By.TAG_NAME, \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1b5f3f6-e1ee-402a-bf1f-ea28530459db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract names and links\n",
    "names = [name.text for name in people]\n",
    "links = [link.get_attribute(\"href\") for link in people]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39917f5c-b542-49fe-bc8b-c833986a4448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams, Alma</td>\n",
       "      <td>https://www.govtrack.us/congress/members/alma_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aguilar, Pete</td>\n",
       "      <td>https://www.govtrack.us/congress/members/pete_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Allred, Colin</td>\n",
       "      <td>https://www.govtrack.us/congress/members/colin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Auchincloss, Jake</td>\n",
       "      <td>https://www.govtrack.us/congress/members/jake_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baldwin, Tammy</td>\n",
       "      <td>https://www.govtrack.us/congress/members/tammy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name                                               info\n",
       "0        Adams, Alma  https://www.govtrack.us/congress/members/alma_...\n",
       "1      Aguilar, Pete  https://www.govtrack.us/congress/members/pete_...\n",
       "2      Allred, Colin  https://www.govtrack.us/congress/members/colin...\n",
       "3  Auchincloss, Jake  https://www.govtrack.us/congress/members/jake_...\n",
       "4     Baldwin, Tammy  https://www.govtrack.us/congress/members/tammy..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, wrap everything up in a dataframe\n",
    "df = pd.DataFrame({\n",
    "    \"name\": names,\n",
    "    \"info\": links})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a8f436f-2a20-4048-b375-9a80f4c3f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196c1ce-94e3-4eba-9a9a-16f43bb1d69c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
